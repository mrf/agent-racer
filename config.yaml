server:
  port: 8080
  host: "127.0.0.1"
  # Optional: restrict browser origins that can open WebSocket connections
  # allowed_origins:
  #   - "http://localhost:8080"
  #   - "http://127.0.0.1:8080"
  # Optional: require an auth token for API/WS access
  # auth_token: ""

sources:
  claude: true
  codex: false    # Set to true to enable Codex (OpenAI) session monitoring
  gemini: false   # Set to true to enable Gemini CLI session monitoring

monitor:
  poll_interval: 1s
  snapshot_interval: 5s
  broadcast_throttle: 100ms
  session_stale_after: 2m
  completion_remove_after: 8s
  # Optional override for Claude SessionEnd hook markers
  # session_end_dir: "/home/user/.local/state/agent-racer/session-end"
  # CPU threshold (%) for detecting active processing (churning state)
  churning_cpu_threshold: 15.0
  # If true, only consider churning when both CPU and TCP connections are active
  churning_requires_network: false

models:
  # Claude models
  claude-opus-4-5-20251101: 200000
  claude-sonnet-4-5-20250929: 200000
  claude-sonnet-4-20250514: 200000
  claude-haiku-3-5-20241022: 200000
  # Codex (OpenAI) models — these are fallbacks; Codex sessions report
  # model_context_window dynamically from rollout events.
  gpt-5-codex: 272000
  gpt-5.1-codex: 272000
  gpt-5.2-codex: 272000
  codex-mini-latest: 200000
  o4-mini: 200000
  o3: 200000
  gpt-4.1: 1047576
  # Gemini models — these are fallbacks; the Gemini source resolves
  # context windows from model name (all current models use ~1M).
  gemini-2.5-pro: 1048576
  gemini-2.5-flash: 1048576
  gemini-2.0-flash: 1048576
  gemini-3-pro-preview: 1000000
  gemini-3-flash-preview: 1000000
  default: 200000
